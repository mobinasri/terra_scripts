{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook transfers selected data tables along with a list of uris from the current workspace to a new one (can be a new workspace with the autoclass feature enabled). After copying file to the same directory in the new bucket this notebook will create new tables with the modified gs uris. The name of the new tables will end with \"_transferred\". They can then be imported to the new workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions, load packages and workspace info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install terra_notebook_utils terra_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from terra_notebook_utils import WORKSPACE_NAME\n",
    "import terra_pandas as tp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import random \n",
    "import statistics as stats\n",
    "import matplotlib.patches as mplpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is adapted from \n",
    "# https://cloud.google.com/storage/docs/samples/storage-copy-file#storage_copy_file-python\n",
    "def copy_blob(transfer_tuple):\n",
    "    \"\"\"Copies a blob from one bucket to another with a new name.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # blob_name = \"your-object-name\"\n",
    "    # destination_bucket_name = \"destination-bucket-name\"\n",
    "    # destination_blob_name = \"destination-object-name\"\n",
    "    bucket_name, blob_name, destination_bucket_name, destination_blob_name = transfer_tuple\n",
    "\n",
    "        \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    source_bucket = storage_client.bucket(bucket_name)\n",
    "    source_blob = source_bucket.blob(blob_name)\n",
    "    destination_bucket = storage_client.bucket(destination_bucket_name)\n",
    "    destination_blob = destination_bucket.blob(blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to copy is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    # There is also an `if_source_generation_match` parameter, which is not used in this example.\n",
    "    #destination_generation_match_precondition = 0\n",
    "\n",
    "    #blob_copy = source_bucket.copy_blob(\n",
    "    #    source_blob, destination_bucket, destination_blob_name)\n",
    "    \n",
    "    rewrite_token = None\n",
    "    rewrite_token, bytes_rewritten, bytes_to_rewrite = destination_blob.rewrite(source_blob)\n",
    "    \n",
    "    while rewrite_token is not None:\n",
    "        rewrite_token, bytes_rewritten, bytes_to_rewrite = destination_blob.rewrite(source_blob, token=rewrite_token)\n",
    "        print(f'Progress so far: {bytes_rewritten}/{bytes_to_rewrite} bytes.')\n",
    "\n",
    "    return get_object_name(blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are taken from \n",
    "# https://github.com/mobinasri/terra_scripts/blob/main/pull_terra_table.py\n",
    "def get_time():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def get_bucket_name(uri):\n",
    "    return uri.split(\"/\")[2]\n",
    "\n",
    "def get_blob_name(uri):\n",
    "    prefix_len = len(get_bucket_name(uri)) + 5\n",
    "    return uri[prefix_len + 1:]\n",
    "\n",
    "def get_object_name(uri):\n",
    "    return uri.split(\"/\")[-1]\n",
    "\n",
    "def is_external(uri, workspace_bucket_name):\n",
    "    return get_bucket_name(uri) != workspace_bucket_name\n",
    "\n",
    "\n",
    "def get_size_uri(uri):\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name = get_bucket_name(uri)\n",
    "        blob_name = get_blob_name(uri)\n",
    "        object_name = get_object_name(uri)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.get_blob(blob_name)\n",
    "        return blob.size\n",
    "    except Exception as e:\n",
    "        print(\"Error for \", uri)\n",
    "        print(e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_existant_files(uris):\n",
    "    kept_uris = []\n",
    "    for uri in uris:\n",
    "        if get_size_uri(uri):\n",
    "            kept_uris.append(uri)\n",
    "    return kept_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Google billing project name and workspace name\n",
    "PROJECT = os.environ['WORKSPACE_NAMESPACE']\n",
    "WORKSPACE =os.path.basename(os.path.dirname(os.getcwd()))\n",
    "bucket = os.environ['WORKSPACE_BUCKET'] + \"/\"\n",
    "\n",
    "# Verify that we've captured the environment variables\n",
    "print(\"Billing project: \" + PROJECT)\n",
    "print(\"Workspace: \" + WORKSPACE)\n",
    "print(\"Workspace storage bucket: \" + bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_uris_from_file(gs_uri):\n",
    "    if gs_uri == \"\" or gs_uri == None:\n",
    "        return []\n",
    "    !gsutil cp {gs_uri} other_uris.txt\n",
    "    with open(\"other_uris.txt\", \"r\") as f:\n",
    "        return f.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs for the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the only code block that has be filled with proper variables. You can freely run the remaining code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the name of the new bucket\n",
    "new_bucket = \"gs://fc-dc39cbb8-a30e-4cd6-b/\"\n",
    "\n",
    "# Which tables you want to transfer ?\n",
    "# Please note that the uris from other \n",
    "# buckets/workspaces will not be transfered\n",
    "table_names_to_transfer = [\"table_2\", \"table_1\"]\n",
    "\n",
    "# Take a list of uris that should be transfered but \n",
    "# may not exist in the given tables\n",
    "other_uris_to_transfer = read_uris_from_file(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse tables and transferrable URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse desired tables\n",
    "tables = []\n",
    "for name in table_names_to_transfer:\n",
    "    tables.append(tp.table_to_dataframe(table_name = name, \n",
    "                                        workspace = WORKSPACE, \n",
    "                                        workspace_namespace = PROJECT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_entities(entities):\n",
    "    elements = []\n",
    "    for entity in entities:\n",
    "        if isinstance(entity, list):\n",
    "            elements.extend(entity)\n",
    "        else:\n",
    "            elements.append(entity)\n",
    "    return np.array(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uris_from_this_bucket(elements, bucket):\n",
    "    if len(elements) == 0: return []\n",
    "    # keep only the files in this bucket\n",
    "    is_in_this_bucket = np.char.startswith(elements, bucket)\n",
    "    uris = elements[is_in_this_bucket]\n",
    "    return uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract uris to transfer\n",
    "uris_to_transfer = []\n",
    "\n",
    "for table in tables:\n",
    "    # flatten table to get all entities\n",
    "    entities = table.to_numpy().flatten()\n",
    "    \n",
    "    # get all elements \n",
    "    # (if an entity is a list it will be \n",
    "    # flattened before appending to the \n",
    "    # output list of elements)\n",
    "    elements = flatten_entities(entities)\n",
    "    \n",
    "    # keep uris from this bucket\n",
    "    uris_to_transfer.extend(get_uris_from_this_bucket(elements, bucket))\n",
    "\n",
    "# Add other uris given in a separate list\n",
    "uris_to_transfer.extend(other_uris_to_transfer)\n",
    "\n",
    "uris_to_transfer = keep_existant_files(uris_to_transfer)\n",
    "\n",
    "# Show some uris\n",
    "print(\"First 10 uris:\")\n",
    "uris_to_transfer[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = sum([get_size_uri(uri) for uri in uris_to_transfer])\n",
    "print(f\"Total size of the objects to be dowloaded = {total_size/1e9} GB (Count = {len(uris_to_transfer)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a list of inputs to pass to the copying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_list = []\n",
    "for old_uri in uris_to_transfer:\n",
    "    new_uri = old_uri.replace(bucket, new_bucket)\n",
    "    new_bucket_name = get_bucket_name(new_uri)\n",
    "    old_bucket_name = get_bucket_name(old_uri)\n",
    "    blob_name = get_blob_name(old_uri)\n",
    "    transfer_list.append((old_bucket_name, blob_name, new_bucket_name, blob_name))\n",
    "# remove redundant uris\n",
    "transfer_set = set(transfer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start transferring data. It may take a while depending on the size of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferred_count = 0\n",
    "threads = 4\n",
    "# make a pool of threads for downloading files in parallel\n",
    "with ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "    futures = [executor.submit(copy_blob, x) for x in transfer_set]\n",
    "    for future in as_completed(futures):\n",
    "        transferred_count += 1\n",
    "        print(f\"Object ({transferred_count}/{len(transfer_set)}) is transfered:\\t{future.result()}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make new tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new list of tables after modifying the uris to point to the new bucket. It should leave uris from other buckets and numeric entities not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_tables = [table.copy(deep=True)for table in tables]\n",
    "for table in copied_tables:\n",
    "    for column in table.columns:\n",
    "        for row in table.index:\n",
    "            entity = table[column][row]\n",
    "            if isinstance(entity, str) and np.char.startswith(entity, bucket):\n",
    "                table[column][row] = entity.replace(bucket, new_bucket)\n",
    "            elif isinstance(entity, list):\n",
    "                for i, element in enumerate(entity):\n",
    "                    if isinstance(element, str) and np.char.startswith(element, bucket):\n",
    "                        table[column][row][i] = element.replace(bucket, new_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save new tables with their names ending with \"_transferred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in copied_tables:\n",
    "    table_name = table.index.name[:-3] # remove \"_id\" from the end\n",
    "    new_table_name = f\"{table_name}_transferred\"\n",
    "    upload_df = table.rename(index={'1': new_table_name + \"_id\"})\n",
    "    tp.dataframe_to_table(table_name = new_table_name, \n",
    "                          df = upload_df,\n",
    "                          workspace = WORKSPACE,\n",
    "                          workspace_namespace = PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new list of other uris after modifying the uris to point to the new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"other_uris_new_bucket.txt\", \"w+\") as f:\n",
    "    if len(other_uris_to_transfer) > 0:\n",
    "        for uri in other_uris_to_transfer:\n",
    "            new_uri = uri.replace(bucket, new_bucket)\n",
    "            f.write(f\"{new_uri}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat other_uris_new_bucket.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
